{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42d9279",
   "metadata": {},
   "source": [
    "# 🚗 Simple YOLO Training for Cars\n",
    "\n",
    "Train YOLO to detect cars, trucks, people, and traffic signs using BDD100K dataset.\n",
    "\n",
    "**What you need:**\n",
    "- Google Colab with GPU enabled\n",
    "- Kaggle account (free)\n",
    "- 30-60 minutes\n",
    "\n",
    "**What this does:**\n",
    "- Downloads BDD100K dataset from Kaggle\n",
    "- Trains YOLO model (fast version)\n",
    "- Exports trained model for download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb2829",
   "metadata": {},
   "source": [
    "## 1. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da5f711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ziade\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU found: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# Check if we have GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU found: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"❌ No GPU! Go to Runtime > Change runtime type > GPU\")\n",
    "    \n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5daea",
   "metadata": {},
   "source": [
    "## 2. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8510ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing...\n",
      "✅ Done!\n",
      "✅ Done!\n"
     ]
    }
   ],
   "source": [
    "# Install what we need\n",
    "print(\"Installing...\")\n",
    "\n",
    "# !pip install ultralytics kagglehub -q\n",
    "\n",
    "# Import libraries\n",
    "from ultralytics import YOLO\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "# from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "# import kagglehub\n",
    "\n",
    "print(\"✅ Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e6470",
   "metadata": {},
   "source": [
    "## 3. Setup Kaggle (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feffdf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Using existing local BDD100K dataset\n",
      "✅ No Kaggle authentication required for local files\n",
      "🎯 Ready to proceed with dataset conversion and training!\n"
     ]
    }
   ],
   "source": [
    "# Using local dataset - no Kaggle setup needed\n",
    "print(\"📁 Using existing local BDD100K dataset\")\n",
    "print(\"✅ No Kaggle authentication required for local files\")\n",
    "print(\"🎯 Ready to proceed with dataset conversion and training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ef71be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing BDD100K dataset from workspace...\n",
      "Images directory: D:\\Projects\\vision vechlie\\bdd100k\n",
      "Labels directory: D:\\Projects\\vision vechlie\\bdd100k_labels_release\n",
      "✅ Dataset ready!\n",
      "\n",
      "Dataset structure:\n",
      "📁 Images: D:\\Projects\\vision vechlie\\bdd100k\n",
      "  📁 bdd100k/\n",
      "  📁 bdd100k\\images/\n",
      "  📁 bdd100k\\images\\100k/\n",
      "  📁 bdd100k\\images\\10k/\n",
      "  📁 bdd100k\\images\\100k\\test/\n",
      "  📁 bdd100k\\images\\100k\\train/\n",
      "  📁 bdd100k\\images\\100k\\val/\n",
      "  📁 bdd100k\\images\\10k\\test/\n",
      "  📁 bdd100k\\images\\10k\\train/\n",
      "  📁 bdd100k\\images\\10k\\val/\n",
      "\n",
      "📁 Labels: D:\\Projects\\vision vechlie\\bdd100k_labels_release\n",
      "  📄 bdd100k\\labels\\bdd100k_labels_images_train.json\n",
      "  📄 bdd100k\\labels\\bdd100k_labels_images_val.json\n",
      "  📁 bdd100k/\n",
      "  📁 bdd100k\\images/\n",
      "  📁 bdd100k\\images\\100k/\n",
      "  📁 bdd100k\\images\\10k/\n",
      "  📁 bdd100k\\images\\100k\\test/\n",
      "  📁 bdd100k\\images\\100k\\train/\n",
      "  📁 bdd100k\\images\\100k\\val/\n",
      "  📁 bdd100k\\images\\10k\\test/\n",
      "  📁 bdd100k\\images\\10k\\train/\n",
      "  📁 bdd100k\\images\\10k\\val/\n",
      "\n",
      "📁 Labels: D:\\Projects\\vision vechlie\\bdd100k_labels_release\n",
      "  📄 bdd100k\\labels\\bdd100k_labels_images_train.json\n",
      "  📄 bdd100k\\labels\\bdd100k_labels_images_val.json\n"
     ]
    }
   ],
   "source": [
    "# Use existing BDD100K dataset\n",
    "print(\"Using existing BDD100K dataset from workspace...\")\n",
    "\n",
    "# Set paths to your existing dataset\n",
    "dataset_base = Path(\"D:/Projects/vision vechlie\")\n",
    "bdd100k_images = dataset_base / \"bdd100k\"\n",
    "bdd100k_labels = dataset_base / \"bdd100k_labels_release\"\n",
    "\n",
    "print(f\"Images directory: {bdd100k_images}\")\n",
    "print(f\"Labels directory: {bdd100k_labels}\")\n",
    "\n",
    "# Check if directories exist\n",
    "if bdd100k_images.exists() and bdd100k_labels.exists():\n",
    "    dataset_path = bdd100k_labels  # Use labels directory as main path\n",
    "    print(\"✅ Dataset ready!\")\n",
    "    \n",
    "    # Show dataset structure\n",
    "    print(\"\\nDataset structure:\")\n",
    "    print(f\"📁 Images: {bdd100k_images}\")\n",
    "    for item in list(bdd100k_images.rglob('*'))[:10]:\n",
    "        if item.is_dir():\n",
    "            print(f\"  📁 {item.relative_to(bdd100k_images)}/\")\n",
    "        else:\n",
    "            print(f\"  📄 {item.relative_to(bdd100k_images)}\")\n",
    "    \n",
    "    print(f\"\\n📁 Labels: {bdd100k_labels}\")\n",
    "    for item in list(bdd100k_labels.rglob('*.json'))[:5]:\n",
    "        print(f\"  📄 {item.relative_to(bdd100k_labels)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Dataset not found!\")\n",
    "    if not bdd100k_images.exists():\n",
    "        print(f\"  Missing: {bdd100k_images}\")\n",
    "    if not bdd100k_labels.exists():\n",
    "        print(f\"  Missing: {bdd100k_labels}\")\n",
    "    dataset_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e1af1",
   "metadata": {},
   "source": [
    "## 4. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d526745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataset...\n",
      "Train JSON: D:\\Projects\\vision vechlie\\bdd100k_labels_release\\bdd100k\\labels\\bdd100k_labels_images_train.json\n",
      "Val JSON: D:\\Projects\\vision vechlie\\bdd100k_labels_release\\bdd100k\\labels\\bdd100k_labels_images_val.json\n",
      "Images base: D:\\Projects\\vision vechlie\\bdd100k\n",
      "Converting training data...\n",
      "  Reading JSON: D:\\Projects\\vision vechlie\\bdd100k_labels_release\\bdd100k\\labels\\bdd100k_labels_images_train.json\n",
      "  Images base: D:\\Projects\\vision vechlie\\bdd100k\n",
      "  Output to: yolo_training\\train\n",
      "  Loaded 69863 annotations\n",
      "  Loaded 69863 annotations\n",
      "  Processing 100/500...\n",
      "  Processing 100/500...\n",
      "  Processing 200/500...\n",
      "  Processing 200/500...\n",
      "  Processing 300/500...\n",
      "  Processing 300/500...\n",
      "  Processing 400/500...\n",
      "  Processing 400/500...\n",
      "  Processing 500/500...\n",
      "  ✅ Converted 500 images\n",
      "  Processing 500/500...\n",
      "  ✅ Converted 500 images\n",
      "✅ Training: 500 images converted\n",
      "Converting validation data...\n",
      "  Reading JSON: D:\\Projects\\vision vechlie\\bdd100k_labels_release\\bdd100k\\labels\\bdd100k_labels_images_val.json\n",
      "  Images base: D:\\Projects\\vision vechlie\\bdd100k\n",
      "  Output to: yolo_training\\val\n",
      "✅ Training: 500 images converted\n",
      "Converting validation data...\n",
      "  Reading JSON: D:\\Projects\\vision vechlie\\bdd100k_labels_release\\bdd100k\\labels\\bdd100k_labels_images_val.json\n",
      "  Images base: D:\\Projects\\vision vechlie\\bdd100k\n",
      "  Output to: yolo_training\\val\n",
      "  Loaded 10000 annotations\n",
      "  Loaded 10000 annotations\n",
      "  Processing 100/100...\n",
      "  ✅ Converted 100 images\n",
      "✅ Validation: 100 images converted\n",
      "\n",
      "Final check:\n",
      "  Train images ready: 500\n",
      "  Val images ready: 100\n",
      "✅ Dataset conversion successful!\n",
      "  Processing 100/100...\n",
      "  ✅ Converted 100 images\n",
      "✅ Validation: 100 images converted\n",
      "\n",
      "Final check:\n",
      "  Train images ready: 500\n",
      "  Val images ready: 100\n",
      "✅ Dataset conversion successful!\n"
     ]
    }
   ],
   "source": [
    "# Convert BDD100K to YOLO format (simplified)\n",
    "def convert_dataset(json_file, images_base_dir, output_dir, max_images=500):\n",
    "    \"\"\"Convert dataset - using only first 500 images for quick training\"\"\"\n",
    "    \n",
    "    classes = {'person': 0, 'car': 1, 'truck': 2, 'bus': 3, 'traffic light': 4, 'traffic sign': 5}\n",
    "    \n",
    "    print(f\"  Reading JSON: {json_file}\")\n",
    "    print(f\"  Images base: {images_base_dir}\")\n",
    "    print(f\"  Output to: {output_dir}\")\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"  Loaded {len(data)} annotations\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error reading JSON: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    # Create folders\n",
    "    labels_dir = output_dir / 'labels'\n",
    "    images_out_dir = output_dir / 'images'\n",
    "    labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "    images_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    count = 0\n",
    "    processed = 0\n",
    "    \n",
    "    for item in data[:max_images]:  # Only use first images for speed\n",
    "        processed += 1\n",
    "        if processed % 100 == 0:\n",
    "            print(f\"  Processing {processed}/{min(max_images, len(data))}...\")\n",
    "            \n",
    "        name = item['name']\n",
    "        # Find image in the nested structure\n",
    "        image_path = None\n",
    "        for possible_path in images_base_dir.rglob(name):\n",
    "            if possible_path.is_file() and possible_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                image_path = possible_path\n",
    "                break\n",
    "        \n",
    "        if not image_path or not image_path.exists():\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Copy image\n",
    "            shutil.copy2(image_path, images_out_dir / name)\n",
    "            \n",
    "            # Convert labels\n",
    "            labels = []\n",
    "            for label in item.get('labels', []):\n",
    "                if label['category'] in classes:\n",
    "                    box = label.get('box2d')\n",
    "                    if not box:\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert to YOLO format\n",
    "                    x1, y1, x2, y2 = box['x1'], box['y1'], box['x2'], box['y2']\n",
    "                    center_x = (x1 + x2) / 2 / 1280\n",
    "                    center_y = (y1 + y2) / 2 / 720\n",
    "                    width = (x2 - x1) / 1280\n",
    "                    height = (y2 - y1) / 720\n",
    "                    \n",
    "                    class_id = classes[label['category']]\n",
    "                    labels.append(f\"{class_id} {center_x:.6f} {center_y:.6f} {width:.6f} {height:.6f}\")\n",
    "            \n",
    "            # Save labels (even if empty - YOLO needs the txt file)\n",
    "            label_file = labels_dir / f\"{Path(name).stem}.txt\"\n",
    "            with open(label_file, 'w') as f:\n",
    "                f.write('\\n'.join(labels))\n",
    "            count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"  ✅ Converted {count} images\")\n",
    "    return count\n",
    "\n",
    "# Convert dataset using your local files\n",
    "work_dir = Path('./yolo_training')\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if dataset_path:\n",
    "    print(\"Converting dataset...\")\n",
    "    \n",
    "    # Use your existing files\n",
    "    train_json = bdd100k_labels / \"bdd100k\" / \"labels\" / \"bdd100k_labels_images_train.json\"\n",
    "    val_json = bdd100k_labels / \"bdd100k\" / \"labels\" / \"bdd100k_labels_images_val.json\"\n",
    "    images_base = bdd100k_images  # Base directory for images\n",
    "    \n",
    "    print(f\"Train JSON: {train_json}\")\n",
    "    print(f\"Val JSON: {val_json}\")\n",
    "    print(f\"Images base: {images_base}\")\n",
    "    \n",
    "    # Convert training set\n",
    "    train_count = 0\n",
    "    if train_json.exists():\n",
    "        print(\"Converting training data...\")\n",
    "        train_count = convert_dataset(train_json, images_base, work_dir / 'train', max_images=500)\n",
    "        print(f\"✅ Training: {train_count} images converted\")\n",
    "    else:\n",
    "        print(f\"❌ Training JSON not found: {train_json}\")\n",
    "    \n",
    "    # Convert validation set\n",
    "    val_count = 0\n",
    "    if val_json.exists():\n",
    "        print(\"Converting validation data...\")\n",
    "        val_count = convert_dataset(val_json, images_base, work_dir / 'val', max_images=100)\n",
    "        print(f\"✅ Validation: {val_count} images converted\")\n",
    "    else:\n",
    "        print(f\"❌ Validation JSON not found: {val_json}\")\n",
    "    \n",
    "    # Verify conversion worked\n",
    "    train_imgs = list((work_dir / 'train' / 'images').glob('*.jpg')) if (work_dir / 'train' / 'images').exists() else []\n",
    "    val_imgs = list((work_dir / 'val' / 'images').glob('*.jpg')) if (work_dir / 'val' / 'images').exists() else []\n",
    "    \n",
    "    print(f\"\\nFinal check:\")\n",
    "    print(f\"  Train images ready: {len(train_imgs)}\")\n",
    "    print(f\"  Val images ready: {len(val_imgs)}\")\n",
    "    \n",
    "    if len(train_imgs) > 0 and len(val_imgs) > 0:\n",
    "        print(\"✅ Dataset conversion successful!\")\n",
    "    else:\n",
    "        print(\"❌ Dataset conversion failed - no images found\")\n",
    "        print(\"Available JSON files:\")\n",
    "        for json_file in bdd100k_labels.rglob(\"*.json\"):\n",
    "            print(f\"  📄 {json_file}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No dataset path available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5750d22",
   "metadata": {},
   "source": [
    "## 6. Create Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc64ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config created:\n",
      "path: yolo_training\n",
      "train: train/images\n",
      "val: val/images\n",
      "nc: 6\n",
      "names: ['person', 'car', 'truck', 'bus', 'traffic light', 'traffic sign']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create YOLO config\n",
    "classes = ['person', 'car', 'truck', 'bus', 'traffic light', 'traffic sign']\n",
    "\n",
    "config_text = f\"\"\"path: {work_dir}\n",
    "train: train/images\n",
    "val: val/images\n",
    "nc: {len(classes)}\n",
    "names: {classes}\n",
    "\"\"\"\n",
    "\n",
    "config_file = work_dir / 'dataset.yaml'\n",
    "with open(config_file, 'w') as f:\n",
    "    f.write(config_text)\n",
    "    \n",
    "print(\"Config created:\")\n",
    "print(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c8c76",
   "metadata": {},
   "source": [
    "## 7. Train YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RTX 3050 GPU Optimization Setup\n",
    "print(\"🎮 Optimizing for RTX 3050 GPU\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Get GPU information\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    print(f\"🖥️  GPU: {gpu_name}\")\n",
    "    print(f\"💾 Total VRAM: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # RTX 3050 specific optimizations\n",
    "    if \"3050\" in gpu_name:\n",
    "        print(\"✅ RTX 3050 detected - applying memory optimizations\")\n",
    "        \n",
    "        # Set memory fraction to prevent OOM\n",
    "        torch.cuda.set_per_process_memory_fraction(0.85)  # Use 85% of VRAM\n",
    "        \n",
    "        # Enable memory growth\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        \n",
    "        print(\"⚙️  Memory optimizations applied:\")\n",
    "        print(\"  📉 Memory fraction: 85%\")\n",
    "        print(\"  🔄 Split size: 128MB\")\n",
    "    else:\n",
    "        print(\"ℹ️  Non-RTX 3050 GPU detected - using standard settings\")\n",
    "    \n",
    "    # Clear any existing cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"🧹 GPU cache cleared\")\n",
    "    \n",
    "    # Check available memory\n",
    "    available_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"💡 Ready for training with {available_memory:.1f}GB VRAM\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No CUDA GPU found - training will use CPU\")\n",
    "    print(\"⚠️  CPU training will be very slow for YOLO\")\n",
    "\n",
    "print(\"\\n🚀 GPU setup complete - ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa118c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune YOLO v8 model\n",
    "print(\"🚀 Starting YOLOv8 Fine-tuning for Autonomous Vehicle Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify dataset is ready before training\n",
    "train_imgs_check = list((work_dir / 'train' / 'images').glob('*.jpg')) if (work_dir / 'train' / 'images').exists() else []\n",
    "val_imgs_check = list((work_dir / 'val' / 'images').glob('*.jpg')) if (work_dir / 'val' / 'images').exists() else []\n",
    "\n",
    "print(f\"📊 Dataset Status:\")\n",
    "print(f\"  Train images: {len(train_imgs_check)}\")\n",
    "print(f\"  Val images: {len(val_imgs_check)}\")\n",
    "\n",
    "if len(train_imgs_check) == 0:\n",
    "    print(\"❌ No training images found! Please run dataset conversion first.\")\n",
    "else:\n",
    "    print(\"\\n🎮 RTX 3050 GPU Detected - Optimizing for 4GB VRAM\")\n",
    "    \n",
    "    # GPU Memory Check\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"🖥️  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"💾 VRAM: {gpu_memory:.1f} GB\")\n",
    "        \n",
    "        # Clear cache for clean start\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"🧹 GPU cache cleared\")\n",
    "    \n",
    "    print(\"\\n🔄 Loading YOLOv8n pre-trained model...\")\n",
    "    model = YOLO('yolov8n.pt')  # Nano model - best for RTX 3050\n",
    "    \n",
    "    print(f\"✅ Model loaded: YOLOv8n (optimized for RTX 3050)\")\n",
    "    print(f\"🎯 Target classes: {len(['person', 'car', 'truck', 'bus', 'traffic light', 'traffic sign'])}\")\n",
    "    print(f\"🖥️  Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    \n",
    "    print(\"\\n🏋️ Starting RTX 3050 optimized fine-tuning...\")\n",
    "    print(\"⏱️  Estimated time: 25-40 minutes (RTX 3050)\")\n",
    "    print(\"💡 Using memory-efficient settings for 4GB VRAM\")\n",
    "    print(\"📈 Progress will be shown below...\")\n",
    "\n",
    "    try:\n",
    "        # Fine-tuning optimized for RTX 3050 (4GB VRAM)\n",
    "        print(\"🎮 RTX 3050 Optimization Settings:\")\n",
    "        print(\"  💾 4GB VRAM - using memory-efficient parameters\")\n",
    "        print(\"  ⚡ Balanced speed and accuracy for your GPU\")\n",
    "        \n",
    "        results = model.train(\n",
    "            # Dataset\n",
    "            data=str(config_file),\n",
    "            \n",
    "            # Training parameters (RTX 3050 optimized)\n",
    "            epochs=20,              # Reduced for faster training\n",
    "            batch=4,                # Small batch for 4GB VRAM\n",
    "            imgsz=416,              # Smaller input size to save memory\n",
    "            \n",
    "            # Learning rate schedule\n",
    "            lr0=0.002,              # Slightly higher for smaller batch\n",
    "            lrf=0.2,                # Higher final LR factor\n",
    "            warmup_epochs=2,        # Shorter warmup\n",
    "            \n",
    "            # Regularization\n",
    "            weight_decay=0.0005,    # L2 regularization\n",
    "            dropout=0.0,            # No dropout\n",
    "            \n",
    "            # Optimization (memory efficient)\n",
    "            optimizer='SGD',        # More memory efficient than AdamW\n",
    "            momentum=0.937,         # Standard momentum\n",
    "            \n",
    "            # Augmentation (light for speed)\n",
    "            hsv_h=0.01,            # Light hue augmentation\n",
    "            hsv_s=0.5,             # Reduced saturation\n",
    "            hsv_v=0.3,             # Reduced value changes\n",
    "            degrees=0.0,           # No rotation\n",
    "            translate=0.05,        # Minimal translation\n",
    "            scale=0.3,             # Light scale changes\n",
    "            shear=0.0,             # No shearing\n",
    "            perspective=0.0,       # No perspective\n",
    "            flipud=0.0,            # No vertical flip\n",
    "            fliplr=0.5,            # Horizontal flip OK\n",
    "            mosaic=0.8,            # Reduced mosaic\n",
    "            mixup=0.0,             # No mixup\n",
    "            \n",
    "            # Hardware (RTX 3050 specific)\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            workers=2,             # Fewer workers to save memory\n",
    "            \n",
    "            # Memory optimization\n",
    "            amp=True,              # Mixed precision (essential for 4GB)\n",
    "            cache=False,           # Don't cache to save RAM\n",
    "            close_mosaic=5,        # Disable mosaic earlier\n",
    "            \n",
    "            # Output\n",
    "            project='runs/train',\n",
    "            name='bdd100k_rtx3050_v8',\n",
    "            exist_ok=True,\n",
    "            save=True,\n",
    "            save_period=10,        # Save less frequently\n",
    "            \n",
    "            # Early stopping\n",
    "            patience=8,            # Shorter patience\n",
    "            \n",
    "            # Validation\n",
    "            val=True,              # Validate during training\n",
    "            plots=True,            # Generate training plots\n",
    "            \n",
    "            # Memory-focused settings\n",
    "            fraction=1.0,          # Use full dataset\n",
    "            profile=False,         # Don't profile\n",
    "            freeze=None,           # Don't freeze layers\n",
    "            multi_scale=False,     # Single scale only\n",
    "            overlap_mask=False,    # Disable for memory\n",
    "            mask_ratio=4,          # Standard mask ratio\n",
    "            \n",
    "            # Additional RTX 3050 optimizations\n",
    "            rect=True,             # Rectangular training (faster)\n",
    "            cos_lr=False,          # Disable cosine LR (simpler)\n",
    "            label_smoothing=0.0,   # No label smoothing\n",
    "            \n",
    "            # Resume\n",
    "            resume=False           # Fresh training\n",
    "        )\n",
    "\n",
    "        print(\"\\n🎉 RTX 3050 Fine-tuning completed successfully!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # GPU memory usage summary\n",
    "        if torch.cuda.is_available():\n",
    "            memory_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            memory_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            print(f\"🎮 RTX 3050 Memory Usage:\")\n",
    "            print(f\"  📊 Allocated: {memory_allocated:.2f} GB\")\n",
    "            print(f\"  💾 Reserved: {memory_reserved:.2f} GB\")\n",
    "            print(f\"  ✅ Training completed within 4GB VRAM limit\")\n",
    "        \n",
    "        # Get best model path\n",
    "        best_model = results.best if hasattr(results, 'best') else 'runs/train/bdd100k_rtx3050_v8/weights/best.pt'\n",
    "        print(f\"✅ Best model saved: {best_model}\")\n",
    "        \n",
    "        # Display training results\n",
    "        if hasattr(results, 'results_dict'):\n",
    "            metrics = results.results_dict\n",
    "            print(f\"\\n📊 Final Training Metrics (RTX 3050 Optimized):\")\n",
    "            print(f\"  mAP50: {metrics.get('metrics/mAP50(B)', 'N/A'):.3f}\")\n",
    "            print(f\"  mAP50-95: {metrics.get('metrics/mAP50-95(B)', 'N/A'):.3f}\")\n",
    "            print(f\"  Precision: {metrics.get('metrics/precision(B)', 'N/A'):.3f}\")\n",
    "            print(f\"  Recall: {metrics.get('metrics/recall(B)', 'N/A'):.3f}\")\n",
    "        \n",
    "        print(f\"\\n📁 Training outputs saved to:\")\n",
    "        print(f\"  📈 Plots: runs/train/bdd100k_rtx3050_v8/\")\n",
    "        print(f\"  🏆 Best weights: {best_model}\")\n",
    "        print(f\"  📋 Logs: runs/train/bdd100k_rtx3050_v8/\")\n",
    "        \n",
    "        # Clear GPU cache after training\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"🧹 GPU cache cleared for inference\")\n",
    "        \n",
    "        # Store for next cells\n",
    "        globals()['best_model'] = best_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {e}\")\n",
    "        print(\"\\n🔍 RTX 3050 Troubleshooting:\")\n",
    "        print(\"- 🧹 Clear GPU cache: torch.cuda.empty_cache()\")\n",
    "        print(\"- 📉 Reduce batch size further (try batch=2)\")\n",
    "        print(\"- 🖼️  Use smaller image size (try imgsz=320)\")\n",
    "        print(\"- 💾 Close other applications using GPU memory\")\n",
    "        print(\"- 🔄 Restart kernel and try again\")\n",
    "        print(\"- 📊 Check dataset conversion completed successfully\")\n",
    "        print(\"- 📝 Verify config file exists and is valid\")\n",
    "        \n",
    "        # Try to free GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"🧹 GPU cache cleared automatically\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc5e00",
   "metadata": {},
   "source": [
    "## 8. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac60e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Evaluation\n",
    "print(\"🔍 Evaluating Fine-tuned YOLOv8 Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'best_model' in globals():\n",
    "    # Load the best trained model\n",
    "    trained_model = YOLO(best_model)\n",
    "    print(f\"📥 Loaded model: {best_model}\")\n",
    "    \n",
    "    # Run validation on full validation set\n",
    "    print(\"\\n📊 Running comprehensive validation...\")\n",
    "    val_results = trained_model.val(data=str(config_file), imgsz=640, batch=16)\n",
    "    \n",
    "    print(f\"\\n🎯 Model Performance Metrics:\")\n",
    "    print(f\"  📈 mAP50: {val_results.box.map50:.4f}\")\n",
    "    print(f\"  📈 mAP50-95: {val_results.box.map:.4f}\")\n",
    "    print(f\"  🎯 Precision: {val_results.box.mp:.4f}\")\n",
    "    print(f\"  🔄 Recall: {val_results.box.mr:.4f}\")\n",
    "    \n",
    "    # Per-class performance\n",
    "    class_names = ['person', 'car', 'truck', 'bus', 'traffic light', 'traffic sign']\n",
    "    print(f\"\\n📋 Per-Class Performance:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        if i < len(val_results.box.ap50):\n",
    "            ap50 = val_results.box.ap50[i]\n",
    "            print(f\"  {class_name:<15}: AP50 = {ap50:.3f}\")\n",
    "    \n",
    "    # Test on sample validation images\n",
    "    val_images = list((work_dir / 'val' / 'images').glob('*.jpg'))\n",
    "    \n",
    "    if val_images:\n",
    "        print(f\"\\n🖼️ Testing on sample images...\")\n",
    "        import random\n",
    "        \n",
    "        # Test on 3 random validation images\n",
    "        test_samples = random.sample(val_images, min(3, len(val_images)))\n",
    "        \n",
    "        for i, test_image in enumerate(test_samples, 1):\n",
    "            print(f\"\\n🔍 Sample {i}: {test_image.name}\")\n",
    "            \n",
    "            # Run prediction\n",
    "            results = trained_model(test_image, conf=0.25, iou=0.45)\n",
    "            \n",
    "            # Show results\n",
    "            detections = results[0].boxes\n",
    "            if detections is not None and len(detections) > 0:\n",
    "                print(f\"  🚗 Found {len(detections)} objects:\")\n",
    "                for j, box in enumerate(detections):\n",
    "                    class_id = int(box.cls[0])\n",
    "                    conf = float(box.conf[0])\n",
    "                    if class_id < len(class_names):\n",
    "                        class_name = class_names[class_id]\n",
    "                        print(f\"    {j+1}. {class_name}: {conf:.3f}\")\n",
    "            else:\n",
    "                print(f\"  ❌ No objects detected\")\n",
    "            \n",
    "            # Save result image\n",
    "            result_path = f'/content/test_result_{i}.jpg'\n",
    "            results[0].save(filename=result_path)\n",
    "            print(f\"  💾 Result saved: test_result_{i}.jpg\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\n✅ Model Evaluation Complete!\")\n",
    "    print(f\"🎯 This model can detect:\")\n",
    "    print(f\"  🚶 People/Pedestrians\")\n",
    "    print(f\"  🚗 Cars\")\n",
    "    print(f\"  🚛 Trucks\") \n",
    "    print(f\"  🚌 Buses\")\n",
    "    print(f\"  🚦 Traffic Lights\")\n",
    "    print(f\"  🛑 Traffic Signs\")\n",
    "    \n",
    "    print(f\"\\n🚀 Ready for autonomous vehicle deployment!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No trained model found. Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9272e6",
   "metadata": {},
   "source": [
    "## 9. Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Fine-tuned Model and Results\n",
    "print(\"📦 Preparing fine-tuned YOLOv8 model for download...\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "if 'best_model' in globals():\n",
    "    # Create download directory\n",
    "    download_dir = Path('/content/bdd100k_yolov8_model')\n",
    "    download_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Copy the best model\n",
    "    model_name = 'bdd100k_autonomous_yolov8.pt'\n",
    "    model_path = download_dir / model_name\n",
    "    shutil.copy2(best_model, model_path)\n",
    "    print(f\"✅ Model copied: {model_name}\")\n",
    "    \n",
    "    # 2. Copy training plots and results\n",
    "    results_dir = Path('runs/train/bdd100k_autonomous_v8')\n",
    "    if results_dir.exists():\n",
    "        plots_dir = download_dir / 'training_plots'\n",
    "        plots_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Copy important files\n",
    "        important_files = ['results.png', 'confusion_matrix.png', 'F1_curve.png', \n",
    "                          'PR_curve.png', 'P_curve.png', 'R_curve.png']\n",
    "        \n",
    "        for file_name in important_files:\n",
    "            src_file = results_dir / file_name\n",
    "            if src_file.exists():\n",
    "                shutil.copy2(src_file, plots_dir / file_name)\n",
    "                print(f\"📊 Plot copied: {file_name}\")\n",
    "    \n",
    "    # 3. Create model info file\n",
    "    model_info = {\n",
    "        \"model_name\": \"BDD100K Autonomous Vehicle YOLOv8\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"base_model\": \"YOLOv8n\",\n",
    "        \"dataset\": \"BDD100K (Berkeley DeepDrive)\",\n",
    "        \"classes\": [\"person\", \"car\", \"truck\", \"bus\", \"traffic light\", \"traffic sign\"],\n",
    "        \"num_classes\": 6,\n",
    "        \"input_size\": [640, 640],\n",
    "        \"training_epochs\": 25,\n",
    "        \"fine_tuned_for\": \"autonomous_vehicle_detection\",\n",
    "        \"use_cases\": [\n",
    "            \"Real-time vehicle detection\",\n",
    "            \"Pedestrian detection\", \n",
    "            \"Traffic sign recognition\",\n",
    "            \"Traffic light detection\",\n",
    "            \"Autonomous driving assistance\"\n",
    "        ],\n",
    "        \"performance_notes\": \"Optimized for autonomous vehicle applications with balanced speed and accuracy\",\n",
    "        \"deployment_info\": {\n",
    "            \"inference_example\": \"model = YOLO('bdd100k_autonomous_yolov8.pt'); results = model('image.jpg')\",\n",
    "            \"confidence_threshold\": 0.25,\n",
    "            \"iou_threshold\": 0.45\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    info_file = download_dir / 'model_info.json'\n",
    "    with open(info_file, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    print(f\"📋 Model info saved: model_info.json\")\n",
    "    \n",
    "    # 4. Create usage example\n",
    "    usage_example = '''# BDD100K Autonomous Vehicle YOLOv8 Usage Example\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model = YOLO('bdd100k_autonomous_yolov8.pt')\n",
    "\n",
    "# Run inference on an image\n",
    "results = model('path/to/your/image.jpg', \n",
    "                conf=0.25,    # Confidence threshold\n",
    "                iou=0.45)     # IoU threshold for NMS\n",
    "\n",
    "# Process results\n",
    "for result in results:\n",
    "    boxes = result.boxes\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            # Get detection info\n",
    "            class_id = int(box.cls[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            class_names = ['person', 'car', 'truck', 'bus', 'traffic light', 'traffic sign']\n",
    "            class_name = class_names[class_id]\n",
    "            \n",
    "            print(f\"Detected: {class_name} (confidence: {confidence:.2f})\")\n",
    "    \n",
    "    # Display annotated image\n",
    "    annotated_img = result.plot()\n",
    "    cv2.imshow('Autonomous Vehicle Detection', annotated_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# For real-time video processing\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        results = model(frame, conf=0.25)\n",
    "        annotated_frame = results[0].plot()\n",
    "        \n",
    "        cv2.imshow('Real-time Detection', annotated_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Usage: process_video('dashcam_video.mp4')\n",
    "'''\n",
    "    \n",
    "    example_file = download_dir / 'usage_example.py'\n",
    "    with open(example_file, 'w') as f:\n",
    "        f.write(usage_example)\n",
    "    print(f\"💡 Usage example saved: usage_example.py\")\n",
    "    \n",
    "    # 5. Download the model\n",
    "    print(f\"\\n⬇️ Downloading fine-tuned model...\")\n",
    "    try:\n",
    "        files.download(str(model_path))\n",
    "        print(f\"✅ Model downloaded successfully!\")\n",
    "        \n",
    "        # Also download the info file\n",
    "        files.download(str(info_file))\n",
    "        print(f\"📋 Model info downloaded!\")\n",
    "        \n",
    "        # Download usage example\n",
    "        files.download(str(example_file))\n",
    "        print(f\"💡 Usage example downloaded!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Download error: {e}\")\n",
    "        print(f\"💡 Files are saved in: {download_dir}\")\n",
    "        print(f\"Use the file browser to download manually\")\n",
    "    \n",
    "    print(f\"\\n🎉 Fine-tuned YOLOv8 model ready!\")\n",
    "    print(f\"📊 Model Performance Summary:\")\n",
    "    print(f\"  ✅ Trained on BDD100K dataset\")\n",
    "    print(f\"  🎯 6 classes for autonomous vehicles\")\n",
    "    print(f\"  ⚡ Optimized for real-time detection\")\n",
    "    print(f\"  🚗 Ready for deployment in vehicles\")\n",
    "    \n",
    "    print(f\"\\n🚀 Next Steps:\")\n",
    "    print(f\"1. Test with your own images/videos\")\n",
    "    print(f\"2. Integrate into your autonomous system\") \n",
    "    print(f\"3. Monitor performance in real scenarios\")\n",
    "    print(f\"4. Consider additional fine-tuning with domain-specific data\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ No trained model available for download\")\n",
    "    print(f\"Please run the training section first to fine-tune the model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
